<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Classifiers</title>
    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
    <style>
      body {
            background-color: #121212;
            color: #FFFFFF;
            font-family: Arial, sans-serif;
            padding: 20px;
        }
        .header {
            text-align: center;
            margin-bottom: 30px;
        }
        table {
            width: 80%; margin-left: auto; margin-right: auto;
            border-collapse: collapse;
            margin-top: 30px;
        }
        th, td {
            padding: 8px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        th {
            background-color: #333333;
            color: #FFFFFF;
        }
        tr:hover {
            background-color: #555555;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Classifiers</h1>
            <p>Classifiers evaluate outputs from LLMs, resulting in binary output from fuzzy.<br/>Below are the available classifiers with their respective 3-gram codes:</p>
        </div>
        <table>
            <tbody>
                <tr>
                    <th>3 gram code</th>
                    <th>Description</th>
                    <th>Notes</th>
                </tr>
                <tr>
                    <td>har (default)</td>
                    <td>Classifier for harmful content using LLM (e.g., 'Is the following content harmful?').</td>
                    <td>Works best with closed-source models (i.e gpt)</td>
                </tr>
                <tr>
                    <td>oai</td>
                    <td>Classifier which uses the OpenAI moderation endpoint.</td>
                    <td>Requires the environment variable OPENAI_API_KEY. Seems to be the most effective</td>
                </tr>
                <tr>
                    <td>rat</td>
                    <td>Classifier for rating content using LLM (e.g., 'Rate the response on a scale from 1 to 10').</td>
                    <td>Works best with closed-source models (i.e gpt)</td>
                </tr>
                <tr>
                    <td>com</td>
                    <td>Committee classifier which uses multiple prompts to classify the input.</td>
                    <td>Works best with closed-source models (i.e gpt)</td>
                </tr>
                <tr>
                    <td>dis</td>
                    <td>Classifier for disapproval content using LLM (e.g., 'I'm sorry but I can't help you').</td>
                    <td>Works best with closed-source models (i.e gpt)</td>
                </tr>
                <tr>
                    <td>cos</td>
                    <td>Classifier utilizing cosine similarity between two sentences. The attack handler must be specifically designed to address classifiers of this nature</td>
                    <td></td>
                </tr>
                <tr>
                    <td><a href="classifiers/gen.html" target="_blank">gen</a></td>
                    <td>Generic classifier for LLM evaluator (define your own prompt).</td>
                    <td></td>
                </tr>
                <tr>
                    <td>sen</td>
                    <td>Classifier for harmful content using Sentiment analysis/LLM.</td>
                    <td></td>
                </tr>
                <tr>
                    <td>res</td>
                    <td>Uses zero-shot learning to analyze the sentiment of the response.</td>
                    <td></td>
                </tr>
                <tr>
                    <td>obv</td>
                    <td>Classifier that returns True if the text does not start with an obvious negative response.</td>
                    <td></td>
                </tr>
            </tbody>
        </table>
    </div>
</body>
</html>
